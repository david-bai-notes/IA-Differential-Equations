\section{Higher Order Linear ODEs}
\subsection{Second Order Linear ODEs with Constant Coefficients}
\begin{definition}
    A second order linear ODE with constant coefficient is an ODE of the form
    $$a\frac{\mathrm d^2y}{\mathrm dx^2}+b\frac{\mathrm dy}{\mathrm dx}+cy=f(x)$$
    where $a,b,c$ are constants with $a\neq 0$.
\end{definition}
\begin{definition}
    A linear differential operator $\mathscr{D}$ is a linear combination of (different orders of) differentiation operators.
\end{definition}
We know that the (arbitrary order) differentiation operator is linear, hence any linear differential operator is linear, which gives rise to the principle of superposition.
\begin{proposition}
    If $y_\alpha, y_\beta$ are solutions to $ay^{\prime\prime}+by^\prime+cy=f$, then $y_{\alpha}-y_\beta$ is a solution to $ay^{\prime\prime}+by^\prime+cy=0$.
\end{proposition}
\begin{proof}
    Consider the linear operator
    $$\mathscr{D}=a\frac{\mathrm d^2}{\mathrm dx^2}+b\frac{\mathrm d}{\mathrm dx}+c$$
    then $\mathscr{D}(y_\alpha-y_\beta)=\mathscr{D}(y_\alpha)-\mathscr{D}(y_\beta)=f(x)-f(x)=0$.
\end{proof}
We can extend the above proposition to any order of linear ODEs with constant coefficient in the obvious way.\\
So we can solve these solutions in the following steps, assuming $\mathscr{D}$ is defined as in the above proof:\\
First, we find complementary (linearly independent, defined below) solutions $y_1,y_2$ to $\mathscr{D}(y)=0$.\\
Then we find a particular solution $y_p$ to $\mathscr{D}(y)=f(x)$.\\
The general form of the solutions to $\mathscr{D}(y)=f(x)$ is $y_p+Ay_1+By_2$ where $A,B$ are constants.
\begin{definition}
    A set of functions $(f_i)_{i\in I}$ are linearly dependent if $\sum_ic_if_i(x)=0$ for some constants $c_i$ that are not all zero.
    The sum here is taken over some finite set of indices.\\
    Otherwise, they are linearly independent.
\end{definition}
Equivalently, if a function in the set of functions can be written as a linear combination of others, then the set is linearly dependent.\\
Consider a second-order linear differential operator.
We know that the a first order one has eigenfunction to be the exponential function.
Note that it is also the eigenfunction of a second-order one.
In fact, the exponential is the eigenfunction of any linear differential operator.
Consider the homogeneous equation $\mathscr{D}y=0$ where
$$\mathscr{D}=a\frac{\mathrm d^2}{\mathrm dx^2}+b\frac{\mathrm d}{\mathrm dx}+c$$
Plug in $y=e^{\lambda x}$ we have $a\lambda^2+b\lambda+c=0$ which we call \textit{characteristic equation} or \textit{auxiliary equation}.
From FTA, we have at least $1$ (complex) solutions.
Let $\lambda_1,\lambda_2$ be two solutions, then\\
Case 1: $\lambda_1\neq\lambda_2$.\\
Then $y_1=Ae^{\lambda_1x}$, $y_2=Be^{\lambda_2x}$ are both solutions for each $A,B$ constants.
It is easy to see that these two are linearly independent for $AB\neq 0$.
We can show that they form a basis for solution space,
\footnote{This will be discussed later.}
and any other solutions must be of the form $Ae^{\lambda_1x}+Be^{\lambda_2x}$ for $A,B$ constants.\\
Case 2: $\lambda_1=\lambda_2$.\\
In this case, $y_1,y_2$ cannot span the solution space, but there is some workaround.
\begin{example}
    $y^{\prime\prime}-4y^\prime+4y=0$, then $(y^\prime-2y)^\prime-2(y^\prime-2y)=0$ can give the solution.\\
    Or we can consider the slightly modifies equation $y^{\prime\prime}-4y^\prime+(4-\epsilon^2)y=0$ for some small enough $\epsilon$.
    This modified equation has the general solution $y_\epsilon=Ae^{\lambda_1x}+Be^{\lambda_2x}$ where $\lambda_{1,2}=2\pm\epsilon$, so
    $$y_\epsilon=e^{2x}(Ae^{\epsilon x}+Be^{-\epsilon x})=e^{2x}((A+B)+\epsilon x(A-B)+O(\epsilon^2))\to Cxe^{2x}+De^{2x}$$
    for some constants $C,D$ by clever (or not) choices of $A,B$.
    This gives a pair of linearly independent solutions.
\end{example}
\subsection{Second Order Linear ODEs with Non-constant Coefficients}
Again we are interested in the homogeneous ones due to the superposition principle.
Consider the equations in the form
$$y^{\prime\prime}+p(x)y^\prime+q(x)y=0$$
We shall use the method of reduction of order.
Given one solution $y_1$ to the equation, we shall find a second solution by looking for solutions of the form $y_2=vy_1$.\\
First, note that $y_2^\prime=v^\prime y_1+vy_1^\prime,y_2^{\prime\prime}=v^{\prime\prime}y_1+2v^\prime y_1^\prime +vy^{\prime\prime}$, so plugging it in we have
$$v^\prime(2y_1^\prime+py_1)+v^{\prime\prime}y_1=0$$
So it is a seperable first order equation in $v^\prime$ which we know how to solve, plugging it back gives the solution.
\subsection{Phase Space}
Consider the ODE $p_i(x)y^{(i)}=f(x)$ where the summation convention is used and $i$ is summed over $0,1,\ldots,n$.
So we can write $y^{(n)}$ as a combination of $y^{(i)}$ for $i\in\{0,1,2,\ldots,n-1\}$ and $f$ and $p_i$'s.
\begin{example}
    The damped oscillator has the DE
    $$m\ddot{y}=-ky-L\dot{y}$$
    The state of the system can be described by an $n$-dimensional solution vector
    $$\underline{y}=
    \begin{pmatrix}
        y\\
        y^\prime\\
        \vdots\\
        y^{(n-1)}
    \end{pmatrix}$$
    Going back to an undampted oscillator $y^{\prime\prime}+4y=0$ which has the general solution spanned by $y_1(x)=\cos{2x}, y_2(x)=\sin{2x}$, so the solution vectors are
    $$\underline{y_1}=
    \begin{pmatrix}
        \cos{2x}\\
        -2\sin{2x}
    \end{pmatrix},
    \underline{y_2}=
    \begin{pmatrix}
        \sin{2x}\\
        2\cos{2x}
    \end{pmatrix}$$
    Thus we can do 2D phase portrait of the solutions to observe the two vectors, where we can find that the trajectories coincide.
    Since $y_1,y_2$ are linearly independent, any point in phase space can be obtained from a linear combination of them.
    In general, $y_1,y_2,\ldots, y_n$ are linearly independent if their solution vectors are linearly independent in the phase space.
\end{example}
$n$ linearly independent solution vectors form a basis for the phase space of an $n^{th}$ order ODE.
Consider the initial conditions for a second order homogeneous ODE $y(0)=a,y^\prime(0)=b$.
If the general solution is formed by the linear combination of linearly independent functions $y_1,y_2$, then in order to find a solution that complies with the initial condition, we will be solving the linear system in $A,B$.
$$
\begin{cases}
    Ay_1(0)+By_2(0)=a\\
    Ay_1^\prime(0)+By_2^\prime(0)=b\\
\end{cases}
$$
So we obtain unique solutions if and only if
$$y_1(0)y_2^\prime(0)\neq y_2(0)y_1^\prime(0)$$
which is true if $\underline{y_1}(0),\underline{y_2}(0)$ are linearly independent.
\begin{definition}
    The Wronskian $W(x)$ is defined as
    $$W(x)=
    \begin{vmatrix}
        y_1&y_2&\dots&y_n\\
        y_1^\prime&y_2^\prime&\dots&y_n^\prime\\
        \vdots&\vdots&\ddots&\vdots\\
        y_1^{(n-1)}&y_2^{(n-1)}&\dots&y_n^{(n-1)}
    \end{vmatrix}
    $$
\end{definition}
So the solutions are linearly independent if $W(x)\neq 0$.
But does $W(x)=0$ necessarily imply linear dependence?
\begin{theorem}[Abel's Theorem]
    Consider a second order linear ODE $y^{\prime\prime}+py^\prime+qy=0$.
    If $p(x),q(x)$ are continuous on an interval $I$, then either $\forall x\in I, W(x)\neq 0$ or $\forall x\in I, W(x)=0$
\end{theorem}
\begin{proof}[Sketch of proof]
    Let $y_1,y_2$ be solutions to the ODE, and $\mathscr D$ be the differential operator in the left hand side of the equation, so
    $$y_2\mathscr{D}(y_1)=y_1\mathscr{D}(y_2)=0\implies y_2y_1^{\prime\prime}-y_1y_2^{\prime\prime}+(y_2y_1^\prime-y_1y_2^\prime)p\implies W^\prime+pW=0$$
    So we could integrate it back from $x_0$ to $x$ to get
    $$W(x)=W(x_0)\exp\left(-\int_{x_0}^xp(u)\,\mathrm du\right)$$
    which is called the Abel identity.
    Since $p$ is continuous on a closed interval, it is bounded and integrable, therefore the exponential function part is always defined and nonzero, so $W(x_0)=0$ if and only if $W(0)=0$.
\end{proof}
\begin{corollary}
    If $p(x)=0$, then the Wronskian is constant.
\end{corollary}
One can generalize the theorem above to $n^{th}$ order linear ODEs.
Indeed, for $\underline{y}^\prime+A(x)\underline{y}=\underline{0}$, we have $W^\prime+\operatorname{tr}(A(x))W=0$.\\
One practical application of Abel's identity can be used to find a second solution $y_2$ given one solution $y_1$.
This can be done by observing that
$$y_1y_2^\prime-y_2y_1^\prime=W(x)=W(x_0)\exp\left(-\int_{x_0}^xp(u)\,\mathrm du\right)$$
is both linear and of first order in $y_2$.\\
We cannot, of course, (analytically) solve all ODEs, but for some special types of them, it is sometimes possible.
\subsection{Special Types of ODEs}
\begin{definition}
    An ODE is called equidimensional if the differential operator is unaffected by a multiplicative rescaling.
    So $\mathscr{D}_x=\mathscr{D}_{\tilde{x}=\alpha x}$ where $\alpha$ is a constant.
    So the general form of a second order linear equidimensional is the following:
    $$ax^2\frac{\mathrm d^2y}{\mathrm dx^2}+bx\frac{\mathrm dy}{\mathrm dx}+cy=f(x)$$
\end{definition}
There are two methods to find $y_c$.\\
Method 1: Note that $y=x^k$ is an eigenfunction of the eigenvector of $x(\mathrm d/\mathrm dx)$.
To solve the homogeneous equation, we can plug the eigenfunction in and find out that $k$ satisfies
$$ak(k-1)+bk+c=0$$
If there are two roots $k_1,k_2$, then the general solution is $y_c=Ax^{k_1}+Bx^{k_2}$ where $A,B$ are constants.
Otherwise, we have at least one $y_c$, so we could use the method in previous sections to find the other.\\
Method 2: Use the substitution $z=\ln x$, so we will have
$$a\frac{\mathrm d^2y}{\mathrm dz^2}+(b-a)\frac{\mathrm dy}{\mathrm dz}+cy=0$$
which we know how to solve.\\
For the forced type of ODE, due to the superposition principle, we just (and will) need a particular solution $y_p$.
There are two methods to do it.\\
Method 1: Guesswork.
\begin{center}
    \begin{tabular}{c|c}
        Form of $f(x)$&Educated guess\\
        \hline
        $e^{kx}$&$Ae^{kx}$\\
        $\sin(kx),\cos(kx)$&$A\sin{kx}+B\cos{kx}$\\
        Polynomial&Polynomials
    \end{tabular}
\end{center}
We could plug the educated guesses in and solve for the coefficients.\\
Method 2: Method of parameters.
Given complementary functions $y_1,y_2$ and solution vectors $\underline{y_1},\underline{y_2}$.
Suppose that the solution vector $\underline{y_p}$ for $y_p$ satisfies
$$\underline{y_p}=u(x)\underline{y_1}+v(x)\underline{y_2}$$
So what we now try to do is to define two equations for $u^\prime,v^\prime$
$$
\begin{cases}
    y_p=uy_1+vy_2\\
    y_p^\prime=uy_1^\prime+vy_2^\prime
\end{cases}
$$
So by differentiating the first equation and comparing it with the other, we have $u^\prime y_1+v^\prime y_2=0$.
Now differentiate the second equation and plug back to the differential equation, we know that $u^\prime y_1^\prime+ v^\prime y_2^\prime =f(x)$.
Since the Wronskian is not zero due to definitions of $y_1,y_2$, we know that we can solve for $u^\prime,v^\prime$ and a unique solution is guaranteed.
Indeed, $u^\prime=-fy_2/W,v^\prime=fy_1/W$.
So
$$y_p=y_2\int_{x_0}^x\frac{y_1(t)f(t)}{W(t)}\,\mathrm dt-y_1\int_{x_0}^x\frac{y_2(t)f(t)}{W(t)}\,\mathrm dt$$
\begin{definition}
    A forced oscillating ODE is a linear second order ODE forced by oscillating force.
\end{definition}
This arises as many physical systems have a restoring force and damping (e.g. friction).
\begin{example}
    A wheel of mass $M$ is connected to a spring and a damper from above and a force $F(t)$ is applied from below, so $M\ddot{y}=F(t)-ky-L\dot{y}$, written in stardard form,
    $$\ddot{y}+\frac{L}{M}\dot{y}+\frac{k}{M}y=\frac{F(t)}{M}$$
    we redefine time $\tau=\sqrt{k/M}t$ and get
    $$y^{\prime\prime}+2Ky^\prime+y=f(\tau), K=\frac{L}{2\sqrt{kM}}$$
\end{example}
We can evaluate the unforced (free, homogeneous) solution where $f\equiv 0$, which we know how to solve in the general form $Ay_1+By_2$ where $A,B$ are constants.\\
Case 1: $K<1$, so we have complex roots of auxiliary equation, so we say the system is underdamped, so
$$y=e^{-K\tau}[A\sin(\sqrt{1-K^2}\tau)+B\cos(\sqrt{1-K^2}\tau)]$$
Case 2: $K=1$, which system we call it is a critically damped.
$$y=(A+B\tau)e^{-K\tau}$$
Case 3: $K>1$, where the system is overdamped.
$$y=Ae^{\lambda_1\tau}+Be^{\lambda_2\tau},\lambda_{1,2}=-K\pm\sqrt{K^2-1}$$
So the unforced response always decays exponentially as time goes to infinity.\\
As for forced response, if we have
$$\ddot{y}+\mu\dot{y}+\omega_0^2y=\sin{\omega t}$$
which by guessing we have the particular solution
$$y_p=\frac{\omega_0^2-\omega^2}{\omega_0^2-\omega^2+\mu^2\omega^2}\sin{\omega t}+\frac{-\mu\omega}{\omega_0^2-\omega^2+\mu^2\omega^2}\cos{\omega t}$$
For $\mu\neq 0$, we have finite complitude oscillations matching the forcing frequency.
Even when $\omega=\omega_0$, then taking the limit to get $-(\cos\omega t)/(\mu\omega)$, in which case we still have finite amplitude oscillations.\\
So in general, in a damped system, the unforced part gives the short time response whilst the particular solution gives the long time behaviour.\\
But if $\mu=0,\omega_0=\omega$, we call this is a resonance.
The forcing then matches the unforced responce, where the equation turns to
$$\ddot{y}+\omega_0 y=\sin\omega_0 t$$
We shall use detuning to solve the equation.
Consider the equation
$$\ddot{y}+\omega_0 y=\sin\omega t$$
So by guessing, we have $y_p=\sin\omega t/(\omega_0^2-\omega_2)$.
Due to linearity, $y_p+Ay_c$ works for any constant $A$, so
$$y_p=\frac{\sin\omega t-\sin\omega_0t}{\omega_0^2-\omega^2}=\frac{2}{\omega_0^2-\omega^2}\cos\left(\frac{\omega_0+\omega}{2}t\right)\sin\left(\frac{\omega-\omega_0}{2}t\right)$$
also solves the equation.
By letting $\omega_0\to\omega$, we have
$$y_p=-\frac{t\cos\omega_0t}{2\omega_0}$$
which indeed solves the problem.
\subsection{Dirac Delta (sorry; not sorry)}
\begin{definition}
    An impulse forcing is a type of forcing by a sudden change.
\end{definition}
\begin{example}
    A mountainbike riding from the road onto a side block.
    When they hits, then there is a sudden increase in altitude, which might be followed by some oscillation.
    When the time for this goes to zero, then it is considered as an (infinite) impulse.
    So the forced, damped oscillator has the equation
    $$M\ddot{y}=F(t)-ky-L\dot{y}$$
    Now if we add the impulse, and we integrate both sides and let the time elapsed during the hitting to the road block tend to $0$, we will want to define the impulse by
    $$I=\lim_{\epsilon\to 0}\int_{T-\epsilon}^{T+\epsilon}F(t)\,\mathrm dt=\lim_{\epsilon\to 0}M[\dot{y}]^{T+\epsilon}_{T-\epsilon}$$
\end{example}
We then introduce the Dirac delta function.
\footnote{And this is the end of the world.}
\begin{definition}
    Fix a family of functions $D(t;\epsilon)$ indexed by $\epsilon$ with the properties:\\
    1. For any $t\neq 0$, we have $\lim_{\epsilon\to0}D(t;\epsilon)=0$ for $t\neq 0$.\\
    2.
    $$\int_{\mathbb R}D(t;\epsilon)\,\mathrm dt=1$$
    We ``define'' the Dirac delta function by $\delta(t)=\lim_{\epsilon\to 0}D(t;\epsilon)$.
\end{definition}
\begin{proposition}
    1. $\forall t\neq 0,\delta(t)=0$.\\
    2. For any $a<0<b$,
    $$\int_a^b\delta(t)\,\mathrm dt=1$$
    3. We have the sampling property.
    For all functions $g(x)$ which are continuous at $x=0$, then
    $$\int_{\mathbb{R}}g(x)\delta(x)\,\mathrm dx=g(0)$$
    In general
    $$\int_a^bg(x)\delta(x-x_0)\,\mathrm dx=
    \begin{cases}
        g(x_0)\text{, if $x_0\in[a,b]$}\\
        0\text{, otherwise}
    \end{cases}$$
\end{proposition}
\begin{proof}
    Ahem.
\end{proof}
\begin{definition}
    The Heaviside step function $H(x)$ is defined by
    $$H(x)=\int_{-\infty}^x\delta(x)\,\mathrm dx=
    \begin{cases}
        1\text{, if $x>0$}\\
        0\text{, if $x<0$}
    \end{cases}$$
    And $H(0)$ is not defined.\\
    It is decreed that $H^\prime(x)=\delta(x)$ ``by FTC''.
    \footnote{I know, I know, stay calm, it is an applied course.}
\end{definition}
\begin{definition}
    The ramp function $r(x)$ is defined by
    $$r(x)=\int_{-\infty}^xH(x)\,\mathrm dx=
    \begin{cases}
        x\text{, when $x\ge 0$}\\
        0\text{, otherwise}
    \end{cases}$$
    By FTC, $r^\prime=H$.
    \footnote{Lol I guess?}
\end{definition}
Consider
$$y^{\prime\prime}+py^\prime+qy=\delta(x)$$
Since $\delta(x)=0$ for all $x\neq 0$, so $y^{\prime\prime}+py^\prime+qy=0$ for $x<0$ and $x>0$.
The highest order derivative inherits the discontinuity from the forcing.
But we would want $y$ to be continuous at $x=0$, so $\lim_{\epsilon\to 0}[y]^{\epsilon}_{-\epsilon}=0$.
And $y^\prime$ would have a jump near $0$, so
$$\lim_{\epsilon\to 0}[y^\prime]^\epsilon_{-\epsilon}=\lim_{\epsilon\to 0}\int_{-\epsilon}^\epsilon y^{\prime\prime}+py^\prime+qy\,\mathrm dx=1$$
We first solve the equations for $x<0$ and $x>0$, so we will have $4$ undetermined constants. and $2$ initial conditions, so we need $2$ more equations, which we can find from the two jump conditions as stated above.
\begin{example}
    Consider $y^{\prime\prime}-y=3\delta(x-\pi/2)$
    with $y=0$ at $x=0,\pi$.
    Note that $y^{\prime\prime}-y=0\implies y=A\sinh x+B\cosh x$
    Our initial conditions then implies
    $$y=
    \begin{cases}
        A\sinh x\text{, when $x<\pi/2$.}\\
        C\sinh (\pi-x)\text{, when $x>\pi/2$.}
    \end{cases}$$
    For constants $A,C$.
    So we put in the jump condition to have
    $3=\int_{\pi/2-\epsilon}^{\pi/2+\epsilon}y^{\prime\prime}-y\,\mathrm dx=[y^\prime]^{(\pi/2)^+}_{(\pi/2)^-}$
    Putting in the definitions to have
    $$-A\cosh(\pi/2)-c\cosh(\pi/2)=3$$
    Since we also have $0=[y]^{(\pi/2)^+}_{(\pi/2)^-}$, we can solve to get $A=C$, therefore
    $$A=C=\frac{-3}{2\cosh(\pi/2)}$$
    Thus
    $$y=
    \begin{cases}
        \frac{-3\sinh x}{2\cosh(\pi/2)}\text{, when $x\le\pi/2$.}\\
        \frac{-3\sinh (\pi-x)}{2\cosh(\pi/2)}\text{, when $x>\pi/2$.}
    \end{cases}$$
\end{example}
We can also have the forcing by Heaviside function.
Consider
$$y^{\prime\prime}+py^\prime+qy=H(x-x_0)$$
For $p,q$ continuous.
$y(x)$ satisfies $y^{\prime\prime}+py^\prime+qy=0$ for $x<x_0$ and $y^{\prime\prime}+py^\prime+qy=1$ for $x>x_0$
We can evaluate the equation on either sides of $x_0$, thus
$$[y^{\prime\prime}]^{x_0^+}_{x_0^-}+p[y^{\prime}]^{x_0^+}_{x_0^-}+q[y]^{x_0^+}_{x_0^-}=1$$
If $y^{\prime\prime}$ behaves like the Heaviside function (then $y^\prime$ bahaves like the ramp function), then $y^\prime,y$ are continuous, thus $[y^{\prime}]^{x_0^+}_{x_0^-}=[y]^{x_0^+}_{x_0^-}=0$ and $[y^{\prime\prime}]^{x_0^+}_{x_0^-}=1$, which is our jump conditions, which would be enough to find out the constants with the initial conditions.
\subsection{Higher-Order Discrete/Difference Equations}
\begin{definition}
    The general form of an $m^{th}$ order linear discrete equation with constant coefficient is
    $$a_my_{n+m}+a_{m-1}y_{n+m-1}+\cdots+a_0y_n=f(n)$$
\end{definition}
Turns out that they are closely related to higher order DEs, and we can solve them using the same principles.
\begin{definition}
    A difference operator $\mathcal D$ is such that $\mathcal D (y_n)=y_{n+1}$.
    It has eigenfunctions in the form $y_n=k^n$ for constant $k$ as $D(k^n)=k(k^n)=ky_n$.
\end{definition}
Note that our difference equation is linear in $y$, thus we can dissolve $y$ into sum of particular and complementary solutions $y_n=y_n^{(c)}+y_n^{(p)}$.
\begin{example}
    We want to solve $a_2y_{n+2}+a_1y_{n+1}+a_0y_n=f_n$.
    Consider the homogeneous equation with $f=0$, then we can put in $y_n=k^n$ to get $a_2k^2+a_1k+a_0=0$, so its solutions $k_{1,2}$ gives the general form of the complementary solution
    $$
    y_n^{(c)}=
    \begin{cases}
        Ak_1^n+Bk_2^n\text{, if $k_1\neq k_2$}\\
        Ak_1^n+Bnk_1^n\text{, if $k_1=k_2$}
    \end{cases}
    $$
    Note that these are all the complementary solutions since it has $2$ degrees of freedom and the sequence would be completely determined by its value at the first two initial values.\\
    We can use guessing works for particular solution
    \begin{center}
        \begin{tabular}{c|c}
            Form of $f_n$&Form of $y_n^{(p)}$\\
            \hline
            $k^n,k\neq k_{1,2}$&$Ak^n$\\
            $k_{1,2}^n$&$Ank_1^n+Bnk_2^n$\\
            Polynomial&Polynomials
        \end{tabular}
    \end{center}
\end{example}
The Fibonacci numbers are defined as $y_0=y_1=1,y_{n+1}=y_n+y_{n-1}$ for $n\ge 1$.
Note that it has the auxiliary equation $k^2-k-1=0$, which has roots $k_{1,2}=(1\pm\sqrt{5})/2$
So by plugging in our initial conditions, we obtain
$$y_n=\frac{1}{\sqrt{5}}(\phi^{n+1}-(-\phi^{-1})^{n+1})$$
where $\phi=(\sqrt{5}+1)/2$ is the golden ratio.
In particular, $y_{n+1}/y_n\to\phi$ as $n\to\infty$.
\subsection{Series Solutions}
Often, there are no analytic solutions to some particular ODE or it is very hard to obtain one.
In this case, we can try to solve the equation in the form of an infinite power series.
We can use the method of Frobenius.
Consider the ODE $py^{\prime\prime}+qy^\prime+ry=0$.
We will seek a series expansion at $x=x_0$ of a (local) solution around some point.
There are many choice of $x_0$.
If the series expansions of $q/p$ and $r/p$ converge locally at $x_0$, we say $x_0$ is an ordinary point.
Otherwise, we say it is a singular point.
There are two types of singular point:
If $x_0$ is a singular point but the equation can be written in the following way:
$$P(x-x_0)^2y^{\prime\prime}+Q(x-x_0)y^\prime+Ry=0$$
and $Q/P,R/P$ are analytic, then we say $x_0$ is a regular singular point.
Note that $Q/P=(x-x_0)q/p,R/P=(x-x_0)^2r/p$.
Otherwise it is called an irregular singular point.
\begin{example}
    1. We want to solve
    $$(1-x^2)y^{\prime\prime}-2xy^\prime+2y=0$$
    then $q/p=-2x/(1-x^2),r/p=2/(1-x^2)$, so $x=\pm 1$ are singularities.
    But $Q/P=(x-x_0)q/p=-2x/(1+x)$, so $x=1$ is regular.
    Similarly $x=-1$ is regular as well.\\
    2. Consider
    $$y^{\prime\prime}\sin x+y^\prime\cos x+2y=0$$
    Then the singularities are $n\pi,n\in\mathbb Z$, but since $(x-n\pi)/(\sin x)$ as $x\to n\pi$ tends to a limit, every of them is regular.\\
    3. We look into
    $$(1+\sqrt{x})y^{\prime\prime}-2xy^\prime+2y=0$$
    So $q/p=-2x/(1+\sqrt{x})$, one can find that the Taylor series at $0$ is undefined.
    Indeed, $0$ is an irregular singular point here.
\end{example}
\begin{theorem}[Fuch's Theorem]
    1. If $x=x_0$ is an ordinary point, then there are two linearly independent power series solutions of the form
    $$y=\sum_{n=0}^\infty a_n(x-x_0)^{n}$$
    locally near $x_0$.\\
    2. If $x=x_0$ is a regular singular point, then there is at least $1$ solution of the form
    $$y=\sum_{n=0}^\infty a_n(x-x_0)^{n+\sigma}$$
    where $\sigma$ is real and $a_0\neq 0$.
\end{theorem}
\begin{example}
    1. The equation $(1-x^2)y^{\prime\prime}-2xy^\prime+2y=0$ has singular points $\pm 1$ and they are both regular.
    We first find series solution about an ordinary point, say $x=0$.
    We try
    $$y=\sum_{n=0}^\infty a_n(x-0)^{n}$$
    So by plugging in,
    $$(1-x^2)\sum_{n=2}^\infty n(n-1)a_nx^{n-2}-2x\sum_{n=1}^\infty na_nx^{n-1}+2\sum_{n=0}^\infty a_nx^{n}=0$$
    From which we have $a_nn(n-1)-a_{n-2}(n-2)(n-3)-2a_{n-2}(n-2)+2a_{n-2}=0$ for $n\ge 2$.
    Just simplify to get $n(n-1)a_n=(n^2-3n)a_{n-2}$, so
    $$a_n=\frac{n-3}{n-1}a_{n-2}$$
    Consequently, $a_0,a_1$, which can be arbitrary constants, could be our initial condition.
    Note that $a_3=0$, hence $a_k=0$ for any odd $k\ge 3$.
    for even values of $n$, we have
    $$a_n=\frac{n-3}{n-1}a_{n-2}=\frac{n-3}{n-1}\frac{n-5}{n-3}a_{n-4}=\frac{n-5}{n-1}a_{n-4}=\cdots=\frac{n-2k-1}{n-1}a_{n-2k}$$
    Thus $a_{2k}=a_0/(1-n)$, so
    $$y=a_1x+a_0\left(1-x^2-\frac{x^4}{3}-\frac{x^6}{5}-\cdots\right)=a_1x+a_0\left( 1-\frac{x}{2}\ln\frac{1+x}{1-x} \right)$$
    2. Consider $4xy^{\prime\prime}+2(1-x^2)y^\prime-xy=0$, which has a regular singular point at $x=0$.
    We now try to expand the solution near it.
    We try $y=\sum_{n=0}^\infty a_nx^{n+\sigma}$ for $a_0\neq 0$ by Fuch's Theorem.
    So we can plug it in our equation to try and get a recurrence for the coefficients.
    Note first that, by multiplying $x$ to both sides
    $$\sum_{n=0}^\infty a_nx^{n+\sigma}(4(n+\sigma)(n+\sigma-1)+2(1-x^2)(n+\sigma)-x^2)=0$$
    Thus by comparing coefficients,
    $$2(n+\sigma)(2n+2\sigma-1)a_n=(2n+2\sigma-3)a_{n-2}$$
    which is our equivalence relation.\\
    To find $\sigma$, we will equate coefficient of lowest power of $x$.
    Set $n=0$, then we can equate the coefficient of $x^\sigma$, so $a_0(4\sigma(\sigma-1))+a_02\sigma=0\implies 2\sigma(2\sigma-1)a_0=0$.
    This is called the indicial equation.
    So we get $\sigma=0$ or $\sigma=1/2$.\\
    For $\sigma=0$, we again set $n=0$ to find that $a_0$ is arbitrary.
    Then consider $n=1$, we have $2a_1=0\implies a_1=0$.
    Our equivalence relation reduced to $2n(2n-1)a_n=(2n-3)a_{n-2}$, which means that $a_k=0$ for any odd $k$.
    For even $n=2k$, we can calculate a few values to get
    $$y=a_0\left(1+\frac{x^2}{4\cdot 3}+\frac{5x^4}{8\cdot 7\cdot 4\cdot 3}+\cdots\right)$$
    For $\sigma=1/2$, our recurrence relation reduced to $(2n+1)(2n)b_n=(2n-2)b_{n-2}$ for $n\ge 2$ where $b_n=a_n$ to avoid ambiguity.
    We can equate the coefficient in the lowest power to get $b_0$ being arbitrary, and by considering $n=1$ we have $b_1=0$, so $b_k=0$ for any odd $k$ as well.
    We can calculate $b_n$ for $n$ even and get
    $$y=b_0x^{1/2}\left( 1+\frac{x^2}{2\cdot 5}+\frac{5x^4}{2\cdot 5\cdot 4\cdot 9}+\cdots \right)$$
\end{example}
There are some special cases of the indicial equation.
Let $x_0$ be a regular singular point, and suppose $\operatorname{Re}(\sigma_1)\le\operatorname{Re}(\sigma_2)$ where $\sigma_{1,2}$ are the roots of the indicial equation.\\
Case 1: $\sigma_2-\sigma_1$ is a non-integer, then the two solutions are linearly independent.\\
Case 2: It is a nonzero integer.
In this case, it is possible, but no guarantee, that the solutions $y_1,y_2$ are linearly dependent, so we might need an extra term in the form $cy_1\ln(x-x_0)$ in $y_2$, where $c$ is a constant.\\
Case 3: It is $0$.
Hence $c\neq 0$, so we can set $c=1$, so we may add $y_1\ln(x-x_0)$ to $y_2$ in order to yield two linearly independent solutions.